- name: Load configuration
  ansible.builtin.import_role:
    name: cluster-config
    allow_duplicates: false

- name: Set dynamic facts
  vars:
    suffix: "{{ cluster.domain | community.dns.get_public_suffix }}"
    main_name: "{{ cluster.domain | replace(suffix, '') | split('.') | last }}"
  ansible.builtin.set_fact:
    k8s_api_host: "{{ cluster.subdomains.k8s_api }}.{{ cluster.domain }}"
    charts_dir: "{{ playbook_dir }}/charts"
    cluster_name: "{{ cluster.domain | replace('.', '-') }}-cluster"
    registered_domain: "{{ main_name }}{{ suffix }}"
    kubeconfig: "{{ kubeconfig if kubeconfig else ansible_env.KUBECONFIG if 'KUBECONFIG' in ansible_env else '/etc/rancher/k3s/k3s.yaml' }}"

- name: Set host kubeconfig
  ansible.builtin.set_fact:
    host_kubeconfig: "{{ kubeconfig }}"

- name: Print cluster.domain info
  ansible.builtin.debug:
    msg: "Setting up cluster '{{ cluster_name }}' for cluster.domain '{{ cluster.domain }}' on registered cluster.domain '{{ registered_domain }}'"

- name: Switch to host cluster context via kubectl
  when: "host_cluster_name"
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Get current kubectl context via command
      ansible.builtin.command: "kubectl config current-context"
      changed_when: false
      register: default_ctx

    - name: Set kubectl context to supplied value
      ansible.builtin.command: "kubectl config use-context {{ host_cluster_name }}"
      changed_when: "host_cluster_name != default_ctx.stdout"

- name: Wait for Kubernetes API to become available
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  ansible.builtin.command: kubectl cluster-info
  register: kubectl_info_result
  retries: 6
  delay: 20
  until: "kubectl_info_result is not failed"
  changed_when: false

- name: Get list of ingress nodes
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  loop: "{{ groups['ingress'] }}"
  kubernetes.core.k8s_info:
    kind: Node
    name: "{{ item }}"
  register: ingress_nodes

- name: Get list of ingress node ips
  vars:
    all_addresses: "{{ ingress_nodes.results | map(attribute='resources') | map('first') | map(attribute='status.addresses') | flatten }}"
  ansible.builtin.set_fact:
    ingress_ips_ext: "{{ all_addresses | selectattr('type', 'equalto', 'ExternalIP') | map(attribute='address') }}"
    ingress_ips_int: "{{ all_addresses | selectattr('type', 'equalto', 'InternalIP') | map(attribute='address') }}"

- name: Get list of zones
  ansible.builtin.set_fact:
    zones: "{{ lookup('ansible.builtin.dict', hostvars) | map(attribute='value.zone', default='') | unique | select() }}"

- name: Ensure cluster labels are applied to nodes
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Add cluster and zone label to inventory node
      loop: "{{ groups['cluster'] }}"
      vars:
        zone: "{{ hostvars[item].zone | default('default') }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels: "{{ {cluster_name: '1', 'topology.kubernetes.io/zone': zone} }}"

    - name: Add backbone labels to inventory node
      loop: "{{ groups['backbone'] }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              topology.kubernetes.io/region: "backbone"
              backbone: "1"

    - name: Add edge region label to inventory node
      loop: "{{ groups['cluster'] | difference(groups['backbone']) }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              topology.kubernetes.io/region: "edge"

    - name: Add cluster load-balancer labels to inventory node
      loop: "{{ groups['ingress'] }}"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              svccontroller.k3s.cattle.io/enablelb: "true"
              svccontroller.k3s.cattle.io/lbpool: "{{ cluster_name }}"

    - name: Add cluster ingress label to inventory node
      loop: "{{ groups['ingress'] }}"
      vars:
        ingress_key: "{{ cluster_name }}-ingress"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels: "{{ {ingress_key: '1'} }}"

    - name: Add cluster storage label to inventory node
      loop: "{{ groups['storage'] }}"
      vars:
        storage_key: "{{ cluster_name }}-storage"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels: "{{ {storage_key: '1'} }}"

- name: Ensure GitOps via FluxCD is set up
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
    KUBECONFIG: "{{ kubeconfig }}"
  block:
    - name: Check if FluxCD release already exists
      kubernetes.core.helm_info:
        name: fluxcd
        release_namespace: flux-system
      register: existing_fluxcd

    - name: Deploy FluxCD
      when: "existing_fluxcd.status is undefined or existing_fluxcd.status.keys() | length == 0"
      kubernetes.core.helm:
        wait: true
        timeout: 10m
        name: fluxcd
        release_namespace: flux-system
        create_namespace: true
        chart_repo_url: https://fluxcd-community.github.io/helm-charts
        chart_ref: flux2
        chart_version: "^2.12.2"
        values:
          policies:
            create: false

    - name: Ensure base config HelmRepository exists
      kubernetes.core.k8s:
        wait: true
        definition:
          apiVersion: source.toolkit.fluxcd.io/v1beta2
          kind: HelmRepository
          metadata:
            name: base-config-repo
            namespace: flux-system
          spec:
            interval: 1h
            url: "{{ cluster.base_config_repo }}"

    - name: Reconcile base config helm repo
      ansible.builtin.command: "flux reconcile source helm base-config-repo -n flux-system"
      changed_when: true

    - name: Ensure base system HelmRepository exists
      kubernetes.core.k8s:
        wait: true
        definition:
          apiVersion: source.toolkit.fluxcd.io/v1beta2
          kind: HelmRepository
          metadata:
            name: base-system-repo
            namespace: flux-system
          spec:
            interval: 1h
            url: "{{ cluster.base_system_repo }}"

    - name: Reconcile base system helm repo
      ansible.builtin.command: "flux reconcile source helm base-system-repo -n flux-system"
      changed_when: true

    - name: Ensure base app HelmRepository exists
      kubernetes.core.k8s:
        wait: true
        definition:
          apiVersion: source.toolkit.fluxcd.io/v1beta2
          kind: HelmRepository
          metadata:
            name: base-app-repo
            namespace: flux-system
          spec:
            interval: 1h
            url: "{{ cluster.base_app_repo }}"

    - name: Reconcile base app helm repo
      ansible.builtin.command: "flux reconcile source helm base-app-repo -n flux-system"
      changed_when: true

- name: Ensure components with CRDs and other hard dependecies are bootstrapped
  ansible.builtin.include_tasks:
    file: bootstrap-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ kubeconfig }}"
  loop:
    - release_name: storage-stack
      chart_name: storage
      namespace: longhorn-system
    - release_name: cert-stack
      chart_name: cert
      namespace: cert-system
    - release_name: telemetry-stack
      chart_name: telemetry
      namespace: telemetry-system

- name: Ensure zoning tags are applied to Longhorn nodes
  environment:
    K8S_AUTH_KUBECONFIG: "{{ host_kubeconfig }}"
  block:
    - name: Pause for 30 seconds to give k8s time to sync storage nodes
      ansible.builtin.pause:
        seconds: 30

    - name: Get list of current Longhorn node tags
      loop: "{{ groups['storage'] }}"
      kubernetes.core.k8s_info:
        api_version: longhorn.io/v1beta2
        kind: Node
        namespace: longhorn-system
        name: "{{ item }}"
      register: lhn_info

    - name: Ensure tags are set on all Longhorn nodes in inventory
      loop: "{{ lhn_info.results }}"
      loop_control:
        label: "{{ item.item }}"
      vars:
        existing_tags: "{{ item.resources[0].spec.tags }}"
        node: "{{ item.resources[0].metadata.name }}"
        region: "{{ hostvars[node].region | default('eu') }}"
        zone: "{{ hostvars[node].zone | default('eu-1') }}"
        cluster_tag: "{{ cluster_name }}"
        region_tag: "{{ ('%s.%s' | format(region, cluster_tag)) }}"
        zone_tag: "{{ ('%s.%s' | format(zone, cluster_tag)) }}"
        backbone_tag: "{{ ('backbone.%s' | format(cluster_tag)) if node in groups['backbone'] else false }}"
        # Assuming here that regions and zones do not share names.
        backbone_region_tag: "{{ ('backbone.%s' | format(region_tag)) if backbone_tag else false }}"
        backbone_zone_tag: "{{ ('backbone.%s' | format(zone_tag)) if backbone_tag else false }}"
        new_tags: "{{ [cluster_tag, backbone_tag, region_tag, zone_tag, backbone_region_tag, backbone_zone_tag] | select() }}"
      kubernetes.core.k8s:
        state: patched
        api_version: longhorn.io/v1beta2
        kind: Node
        namespace: longhorn-system
        name: "{{ node }}"
        definition:
          spec:
            tags: "{{ (existing_tags + new_tags) | unique | list }}"
      register: lhn_patch_result
      retries: 6 # Longhorn CRD webhook tends to be unavailable at times.
      delay: 20
      until: "lhn_patch_result is not failed"

- name: Store cluster config
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: cluster
        namespace: kube-system
      data:
        cluster.yaml: "{{ cluster | to_yaml }}"

- name: Store cluster secrets
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: v1
      kind: Secret
      metadata:
        name: "{{ item.key }}"
        namespace: kube-system
      type: Opaque
      stringData: "{{ item.value }}"
  loop: "{{ cluster_secrets | dict2items }}"
  loop_control:
    label: "{{ item.key }}"

- name: Store S3 secret for etcd backup
  when: "cluster.backup.enabled"
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: v1
      kind: Secret
      metadata:
        name: etcd-s3-config
        namespace: kube-system
      type: Opaque
      stringData:
        etcd-s3-endpoint: "{{ cluster.backup.s3.endpoint }}"
        etcd-s3-access-key: "{{ cluster.backup.s3.access_key_id }}"
        etcd-s3-secret-key: "{{ cluster_secrets['backup-s3'].access_key_secret }}"
        etcd-s3-bucket: "{{ cluster.backup.s3.bucket }}"
        etcd-s3-region: "{{ cluster.backup.s3.region }}"

- name: Ensure remaining infrastructure is fully set up
  ansible.builtin.include_tasks:
    file: deploy-chart.yaml
    apply:
      vars:
        chart: "{{ item }}"
        kubeconfig: "{{ kubeconfig }}"
  loop_control:
    label: "{{ item.release_name }} in {{ item.namespace }}"
  vars:
    node_count: "{{ groups['cluster'] | length }}"
  loop:
    - release_name: storage-stack
      chart_name: storage
      namespace: longhorn-system
      chart_values:
        default_ha_replicas: "{{ [node_count | int, 2] | min }}"
        host: "{{ cluster.subdomains.longhorn }}.{{ cluster.domain }}"
        cluster_name: "{{ cluster_name }}"
        oauth2_proxy_host: "{{ cluster.subdomains.oauth2_proxy }}.{{ cluster.domain }}"
        admin_group: "cluster-admins"
        backup: "{{ cluster.backup }}"
        zones: "{{ zones }}"

    - release_name: dns-stack
      chart_name: dns
      name: dns-stack
      namespace: dns-system
      chart_values:
        domain: "{{ registered_domain }}"
        provider:
          name: "{{ cluster.dns.provider }}"
          extra_config: "{{ cluster.dns.extra_config }}"

    - release_name: k3s-auto-upgrade
      chart_name: upgrade
      namespace: upgrade-system
      chart_values:
        k3s_version: "{{ k3s_version }}"

    - release_name: cert-stack
      chart_name: cert
      namespace: cert-system
      chart_values:
        admin_email: "{{ cluster.mail.admin_address }}"
        base_repo: "{{ cluster.base_repo }}"
        base_branch: "{{ cluster.base_branch }}"
        letsencrypt_staging: "{{ cluster.letsencrypt_staging }}"

    - release_name: ingress-stack
      chart_name: ingress
      namespace: ingress-system
      chart_values:
        cluster_name: "{{ cluster_name }}"

    - release_name: control-stack
      chart_name: control
      name: control-stack
      namespace: kube-system
      chart_values:
        host: "{{ cluster.subdomains.k8s_dashboard }}.{{ cluster.domain }}"
        oauth2_proxy_host: "{{ cluster.subdomains.oauth2_proxy }}.{{ cluster.domain }}"
        admin_group: "cluster-admins"
        cluster_name: "{{ cluster_name }}"

    - release_name: sso-stack
      chart_name: sso
      namespace: sso-system
      chart_values:
        domain: "{{ cluster.domain }}"
        org: "{{ cluster.org }}"
        hosts:
          keycloak: "{{ cluster.subdomains.keycloak }}.{{ cluster.domain }}"
          authproxy: "{{ cluster.subdomains.oauth2_proxy }}.{{ cluster.domain }}"
          account_console: "{{ cluster.subdomains.account_console }}.{{ cluster.domain }}"
        cluster_client:
          id: cluster-oidc
          redirect_uris:
            - "https://{{ cluster.subdomains.grafana }}.{{ cluster.domain }}/login/generic_oauth"
            - "https://{{ cluster.subdomains.weave_gitops }}.{{ cluster.domain }}/oauth2/callback"
        smtp: "{{ cluster.mail.smtp }}"
        admin_group: "cluster-admins"
        admin_email: "{{ cluster.mail.admin_address }}"
        letsencrypt_staging: "{{ cluster.letsencrypt_staging }}"

    - release_name: telemetry-stack
      chart_name: telemetry
      namespace: telemetry-system
      chart_values:
        domain: "{{ cluster.domain }}"
        org: "{{ cluster.org }}"
        host: "{{ cluster.subdomains.grafana }}.{{ cluster.domain }}"
        oidc_client:
          idp_url: "https://{{ cluster.subdomains.keycloak }}.{{ cluster.domain }}/realms/master"
        admin_email: "{{ cluster.mail.admin_address }}"
        admin_group: "cluster-admins"
        smtp: "{{ cluster.mail.smtp }}"
        node_endpoints: "{{ groups['all'] }}"
        letsencrypt_staging: "{{ cluster.letsencrypt_staging }}"
        k3s: true

    - release_name: cicd-stack
      chart_name: gitops
      namespace: flux-system
      chart_values:
        host: "{{ cluster.subdomains.weave_gitops }}.{{ cluster.domain }}"
        oidc_client:
          idp_url: "https://{{ cluster.subdomains.keycloak }}.{{ cluster.domain }}/realms/master"
        letsencrypt_staging: "{{ cluster.letsencrypt_staging }}"

- name: Expose k8s API via Ingress
  environment:
    K8S_AUTH_KUBECONFIG: "{{ kubeconfig }}"
  kubernetes.core.k8s:
    wait: true
    definition:
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        name: k8s-api
        namespace: default
        annotations:
          nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
          nginx.ingress.kubernetes.io/ssl-redirect: "true"
          kubernetes.io/tls-acme: "true"
      spec:
        tls:
          - hosts:
              - "{{ k8s_api_host }}"
            secretName: k8s-api-cert
        rules:
          - host: "{{ k8s_api_host }}"
            http:
              paths:
                - backend:
                    service:
                      name: kubernetes
                      port:
                        number: 443
                  path: /
                  pathType: ImplementationSpecific

- name: Ensure OIDC kubeconfig file is available on local machine
  block:
    - name: Get cluster-oidc secret
      environment:
        KUBECONFIG: "{{ kubeconfig }}"
      kubernetes.core.k8s_info:
        kind: Secret
        namespace: sso-system
        name: cluster-oidc
      register: cluster_oidc_secret
      retries: 3
      delay: 20
      until: "cluster_oidc_secret.resources | length > 0"

    - name: Template out public kubeconfig
      vars:
        api_url: "https://{{ k8s_api_host }}"
        issuer_url: "https://{{ cluster.subdomains.keycloak }}.{{ cluster.domain }}/realms/master"
        client_secret: "{{ cluster_oidc_secret.resources[0].data.secret | b64decode }}"
      ansible.builtin.template:
        src: public-kubeconfig.yaml
        dest: /tmp/public-kubeconfig.yaml
        mode: '0644'

    - name: Check for local kubeconfig
      delegate_to: localhost
      become: false
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/{{ local_kubeconfig }}"
      register: local_kubeconfig_file

    - name: Copy and merge local config file
      when: "local_kubeconfig_file.stat.exists"
      block:
        - name: Copy local kubeconfig to remote
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/{{ local_kubeconfig }}"
            dest: /tmp/local-kubeconfig.yaml
            mode: '0644'

        - name: Delete existing context in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-context {{ cluster_name }}"
          changed_when: true
          failed_when: false

        - name: Delete existing cluster in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-cluster {{ cluster_name }}"
          changed_when: true
          failed_when: false

        - name: Delete existing user in local config
          environment:
            KUBECONFIG: /tmp/local-kubeconfig.yaml
          ansible.builtin.command: "kubectl config delete-user {{ cluster_name }}"
          changed_when: true
          failed_when: false

        - name: Merge local kubeconfig with remote
          environment:
            KUBECONFIG: "/tmp/public-kubeconfig.yaml:/tmp/local-kubeconfig.yaml"
          ansible.builtin.command:
            argv:
              - kubectl
              - config
              - view
              - --flatten
          register: merged_kubeconfig
          changed_when: false

        - name: Save merged kubeconfig to file
          ansible.builtin.copy:
            content: "{{ merged_kubeconfig.stdout }}"
            dest: /tmp/public-kubeconfig.yaml
            mode: '0644'

    - name: Download kubeconfig file
      ansible.builtin.fetch:
        src: /tmp/public-kubeconfig.yaml
        dest: "{{ playbook_dir }}/kubeconfig.yaml"
        flat: true

- name: Ensure idp admin password is available on local machine
  block:
    - name: Get idp-admin secret
      environment:
        KUBECONFIG: "{{ kubeconfig }}"
      kubernetes.core.k8s_info:
        kind: Secret
        namespace: sso-system
        name: idp-admin
      register: idp_admin_secret
      retries: 3
      delay: 20
      until: "idp_admin_secret.resources | length > 0"

    - name: Save idp admin password to file
      ansible.builtin.copy:
        content: "{{ idp_admin_secret.resources[0].data['admin-password'] | b64decode }}"
        dest: /tmp/admin-password
        mode: '0644'

    - name: Download idp admin password
      ansible.builtin.fetch:
        src: /tmp/admin-password
        dest: "{{ playbook_dir }}/clusters/{{ cluster_name }}/idp-admin-pw.txt"
        flat: true

- name: Ensure k3s server token file is available on local machine
  ansible.builtin.fetch:
    src: /var/lib/rancher/k3s/server/token
    dest: "{{ playbook_dir }}/clusters/{{ cluster_name }}/k3s-server-token.txt"
    flat: true

- name: Reset kubectl context
  when: "host_cluster_name and default_ctx is defined"
  environment:
    KUBECONFIG: "{{ host_kubeconfig }}"
  ansible.builtin.command: "kubectl config use-context {{ default_ctx.stdout }}"
  changed_when: "host_cluster_name != default_ctx.stdout"
